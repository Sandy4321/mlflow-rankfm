{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c0ab45-daf9-47b5-9720-f09146b97fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "rankfm main modeling class\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from rankfm._rankfm import _fit, _predict, _recommend\n",
    "from rankfm.utils import get_data\n",
    "import mlflow\n",
    "\n",
    "class RankFM(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"Factorization Machines for Ranking Problems with Implicit Feedback Data\"\"\"\n",
    "    \n",
    "    def __init__(self, factors=10, loss='bpr', max_samples=10, alpha=0.01, beta=0.1, sigma=0.1, learning_rate=0.1, learning_schedule='constant', learning_exponent=0.25):\n",
    "        \"\"\"store hyperparameters and initialize internal model state\n",
    "\n",
    "        :param factors: latent factor rank\n",
    "        :param loss: optimization/loss function to use for training: ['bpr', 'warp']\n",
    "        :param max_samples: maximum number of negative samples to draw for WARP loss\n",
    "        :param alpha: L2 regularization penalty on [user, item] model weights\n",
    "        :param beta: L2 regularization penalty on [user-feature, item-feature] model weights\n",
    "        :param sigma: standard deviation to use for random initialization of factor weights\n",
    "        :param learning_rate: initial learning rate for gradient step updates\n",
    "        :param learning_schedule: schedule for adjusting learning rates by training epoch: ['constant', 'invscaling']\n",
    "        :param learning_exponent: exponent applied to epoch number to adjust learning rate: scaling = 1 / pow(epoch + 1, learning_exponent)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        # validate user input\n",
    "        assert isinstance(factors, int) and factors >= 1, \"[factors] must be a positive integer\"\n",
    "        assert isinstance(loss, str) and loss in ('bpr', 'warp'), \"[loss] must be in ('bpr', 'warp')\"\n",
    "        assert isinstance(max_samples, int) and max_samples > 0, \"[max_samples] must be a positive integer\"\n",
    "        assert isinstance(alpha, float) and alpha > 0.0, \"[alpha] must be a positive float\"\n",
    "        assert isinstance(beta, float) and beta > 0.0, \"[beta] must be a positive float\"\n",
    "        assert isinstance(sigma, float) and sigma > 0.0, \"[sigma] must be a positive float\"\n",
    "        assert isinstance(learning_rate, float) and learning_rate > 0.0, \"[learning_rate] must be a positive float\"\n",
    "        assert isinstance(learning_schedule, str) and learning_schedule in ('constant', 'invscaling'), \"[learning_schedule] must be in ('constant', 'invscaling')\"\n",
    "        assert isinstance(learning_exponent, float) and learning_exponent > 0.0, \"[learning_exponent] must be a positive float\"\n",
    "        \n",
    "        # inherit PythonModel class\n",
    "        mlflow.pyfunc.PythonModel.__init__(self)\n",
    "        \n",
    "        \n",
    "        # store model hyperparameters\n",
    "        self.factors = factors\n",
    "        self.loss = loss\n",
    "        self.max_samples = max_samples\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.sigma = sigma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.learning_schedule = learning_schedule\n",
    "        self.learning_exponent = learning_exponent\n",
    "\n",
    "        # set/clear initial model state\n",
    "        self._reset_state()\n",
    "\n",
    "\n",
    "    # --------------------------------\n",
    "    # begin private method definitions\n",
    "    # --------------------------------\n",
    "\n",
    "\n",
    "    def _reset_state(self):\n",
    "        \"\"\"initialize or reset internal model state\"\"\"\n",
    "\n",
    "        # [ID, IDX] arrays\n",
    "        self.user_id = None\n",
    "        self.item_id = None\n",
    "        self.user_idx = None\n",
    "        self.item_idx = None\n",
    "\n",
    "        # [ID <-> IDX] mappings\n",
    "        self.index_to_user = None\n",
    "        self.index_to_item = None\n",
    "        self.user_to_index = None\n",
    "        self.item_to_index = None\n",
    "\n",
    "        # user/item interactions and importance weights\n",
    "        self.interactions = None\n",
    "        self.sample_weight = None\n",
    "\n",
    "        # set of observed items for each user\n",
    "        self.user_items = None\n",
    "\n",
    "        # [user, item] features\n",
    "        self.x_uf = None\n",
    "        self.x_if = None\n",
    "\n",
    "        # [item, item-feature] scalar weights\n",
    "        self.w_i = None\n",
    "        self.w_if = None\n",
    "\n",
    "        # [user, item, user-feature, item-feature] latent factors\n",
    "        self.v_u = None\n",
    "        self.v_i = None\n",
    "        self.v_uf = None\n",
    "        self.v_if = None\n",
    "\n",
    "        # internal model state indicator\n",
    "        self.is_fit = False\n",
    "\n",
    "\n",
    "    def _init_all(self, interactions, user_features=None, item_features=None, sample_weight=None):\n",
    "        \"\"\"index the interaction data and user/item features and initialize model weights\n",
    "\n",
    "        :param interactions: dataframe of observed user/item interactions: [user_id, item_id]\n",
    "        :param user_features: dataframe of user metadata features: [user_id, uf_1, ..., uf_n]\n",
    "        :param item_features: dataframe of item metadata features: [item_id, if_1, ..., if_n]\n",
    "        :param sample_weight: vector of importance weights for each observed interaction\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        assert isinstance(interactions, (np.ndarray, pd.DataFrame)), \"[interactions] must be np.ndarray or pd.dataframe\"\n",
    "        assert interactions.shape[1] == 2, \"[interactions] should be: [user_id, item_id]\"\n",
    "\n",
    "        # save unique arrays of users/items in terms of original identifiers\n",
    "        interactions_df = pd.DataFrame(get_data(interactions), columns=['user_id', 'item_id'])\n",
    "        self.user_id = pd.Series(np.sort(np.unique(interactions_df['user_id'])))\n",
    "        self.item_id = pd.Series(np.sort(np.unique(interactions_df['item_id'])))\n",
    "\n",
    "        # create zero-based index to identifier mappings\n",
    "        self.index_to_user = self.user_id\n",
    "        self.index_to_item = self.item_id\n",
    "\n",
    "        # create reverse mappings from identifiers to zero-based index positions\n",
    "        self.user_to_index = pd.Series(data=self.index_to_user.index, index=self.index_to_user.values)\n",
    "        self.item_to_index = pd.Series(data=self.index_to_item.index, index=self.index_to_item.values)\n",
    "\n",
    "        # store unique values of user/item indexes and observed interactions for each user\n",
    "        self.user_idx = np.arange(len(self.user_id), dtype=np.int32)\n",
    "        self.item_idx = np.arange(len(self.item_id), dtype=np.int32)\n",
    "\n",
    "        # map the interactions to internal index positions\n",
    "        self._init_interactions(interactions, sample_weight)\n",
    "\n",
    "        # map the user/item features to internal index positions\n",
    "        self._init_features(user_features, item_features)\n",
    "\n",
    "        # initialize the model weights after the user/item/feature dimensions have been established\n",
    "        self._init_weights(user_features, item_features)\n",
    "\n",
    "\n",
    "    def _init_interactions(self, interactions, sample_weight):\n",
    "        \"\"\"map new interaction data to existing internal user/item indexes\n",
    "\n",
    "        :param interactions: dataframe of observed user/item interactions: [user_id, item_id]\n",
    "        :param sample_weight: vector of importance weights for each observed interaction\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        assert isinstance(interactions, (np.ndarray, pd.DataFrame)), \"[interactions] must be np.ndarray or pd.dataframe\"\n",
    "        assert interactions.shape[1] == 2, \"[interactions] should be: [user_id, item_id]\"\n",
    "\n",
    "        # map the raw user/item identifiers to internal zero-based index positions\n",
    "        # NOTE: any user/item pairs not found in the existing indexes will be dropped\n",
    "        self.interactions = pd.DataFrame(get_data(interactions).copy(), columns=['user_id', 'item_id'])\n",
    "        self.interactions['user_id'] = self.interactions['user_id'].map(self.user_to_index).astype(np.int32)\n",
    "        self.interactions['item_id'] = self.interactions['item_id'].map(self.item_to_index).astype(np.int32)\n",
    "        self.interactions = self.interactions.rename({'user_id': 'user_idx', 'item_id': 'item_idx'}, axis=1).dropna()\n",
    "\n",
    "        # store the sample weights internally or generate a vector of ones if not given\n",
    "        if sample_weight is not None:\n",
    "            assert isinstance(sample_weight, (np.ndarray, pd.Series)), \"[sample_weight] must be np.ndarray or pd.series\"\n",
    "            assert sample_weight.ndim == 1, \"[sample_weight] must a vector (ndim=1)\"\n",
    "            assert len(sample_weight) == len(interactions), \"[sample_weight] must have the same length as [interactions]\"\n",
    "            self.sample_weight = np.ascontiguousarray(get_data(sample_weight), dtype=np.float32)\n",
    "        else:\n",
    "            self.sample_weight = np.ones(len(self.interactions), dtype=np.float32)\n",
    "\n",
    "        # create a dictionary containing the set of observed items for each user\n",
    "        # NOTE: if the model has been previously fit extend rather than replace the itemset for each user\n",
    "\n",
    "        if self.is_fit:\n",
    "            new_user_items = self.interactions.groupby('user_idx')['item_idx'].apply(set).to_dict()\n",
    "            self.user_items = {user: np.sort(np.array(list(set(self.user_items[user]) | set(new_user_items[user])), dtype=np.int32)) for user in self.user_items.keys()}\n",
    "        else:\n",
    "            self.user_items = self.interactions.sort_values(['user_idx', 'item_idx']).groupby('user_idx')['item_idx'].apply(np.array, dtype=np.int32).to_dict()\n",
    "\n",
    "        # format the interactions data as a c-contiguous integer array for cython use\n",
    "        self.interactions = np.ascontiguousarray(self.interactions, dtype=np.int32)\n",
    "\n",
    "\n",
    "\n",
    "    def _init_features(self, user_features=None, item_features=None):\n",
    "        \"\"\"initialize the user/item features given existing internal user/item indexes\n",
    "\n",
    "        :param user_features: dataframe of user metadata features: [user_id, uf_1, ... , uf_n]\n",
    "        :param item_features: dataframe of item metadata features: [item_id, if_1, ... , if_n]\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        # store the user features as a ndarray [UxP] row-ordered by user index position\n",
    "        if user_features is not None:\n",
    "            x_uf = pd.DataFrame(user_features.copy())\n",
    "            x_uf = x_uf.set_index(x_uf.columns[0])\n",
    "            x_uf.index = x_uf.index.map(self.user_to_index)\n",
    "            if np.array_equal(sorted(x_uf.index.values), self.user_idx):\n",
    "                self.x_uf = np.ascontiguousarray(x_uf.sort_index(), dtype=np.float32)\n",
    "            else:\n",
    "                raise KeyError('the users in [user_features] do not match the users in [interactions]')\n",
    "        else:\n",
    "            self.x_uf = np.zeros([len(self.user_idx), 1], dtype=np.float32)\n",
    "\n",
    "        # store the item features as a ndarray [IxQ] row-ordered by item index position\n",
    "        if item_features is not None:\n",
    "            x_if = pd.DataFrame(item_features.copy())\n",
    "            x_if = x_if.set_index(x_if.columns[0])\n",
    "            x_if.index = x_if.index.map(self.item_to_index)\n",
    "            if np.array_equal(sorted(x_if.index.values), self.item_idx):\n",
    "                self.x_if = np.ascontiguousarray(x_if.sort_index(), dtype=np.float32)\n",
    "            else:\n",
    "                raise KeyError('the items in [item_features] do not match the items in [interactions]')\n",
    "        else:\n",
    "            self.x_if = np.zeros([len(self.item_idx), 1], dtype=np.float32)\n",
    "\n",
    "\n",
    "    def _init_weights(self, user_features=None, item_features=None):\n",
    "        \"\"\"initialize model weights given user/item and user-feature/item-feature indexes/shapes\n",
    "\n",
    "        :param user_features: dataframe of user metadata features: [user_id, uf_1, ... , uf_n]\n",
    "        :param item_features: dataframe of item metadata features: [item_id, if_1, ... , if_n]\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        # initialize scalar weights as ndarrays of zeros\n",
    "        self.w_i = np.zeros(len(self.item_idx)).astype(np.float32)\n",
    "        self.w_if = np.zeros(self.x_if.shape[1]).astype(np.float32)\n",
    "\n",
    "        # initialize latent factors by drawing random samples from a normal distribution\n",
    "        self.v_u = np.random.normal(loc=0, scale=self.sigma, size=(len(self.user_idx), self.factors)).astype(np.float32)\n",
    "        self.v_i = np.random.normal(loc=0, scale=self.sigma, size=(len(self.item_idx), self.factors)).astype(np.float32)\n",
    "\n",
    "        # randomly initialize user feature factors if user features were supplied\n",
    "        # NOTE: set all user feature factor weights to zero to prevent random scoring influence otherwise\n",
    "        if user_features is not None:\n",
    "            scale = (self.alpha / self.beta) * self.sigma\n",
    "            self.v_uf = np.random.normal(loc=0, scale=scale, size=[self.x_uf.shape[1], self.factors]).astype(np.float32)\n",
    "        else:\n",
    "            self.v_uf = np.zeros([self.x_uf.shape[1], self.factors], dtype=np.float32)\n",
    "\n",
    "        # randomly initialize item feature factors if item features were supplied\n",
    "        # NOTE: set all item feature factor weights to zero to prevent random scoring influence otherwise\n",
    "        if item_features is not None:\n",
    "            scale = (self.alpha / self.beta) * self.sigma\n",
    "            self.v_if = np.random.normal(loc=0, scale=scale, size=[self.x_if.shape[1], self.factors]).astype(np.float32)\n",
    "        else:\n",
    "            self.v_if = np.zeros([self.x_if.shape[1], self.factors], dtype=np.float32)\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # begin public method definitions\n",
    "    # -------------------------------\n",
    "\n",
    "\n",
    "    def fit(self, interactions, user_features=None, item_features=None, sample_weight=None, epochs=1, verbose=False):\n",
    "        \"\"\"clear previous model state and learn new model weights using the input data\n",
    "\n",
    "        :param interactions: dataframe of observed user/item interactions: [user_id, item_id]\n",
    "        :param user_features: dataframe of user metadata features: [user_id, uf_1, ... , uf_n]\n",
    "        :param item_features: dataframe of item metadata features: [item_id, if_1, ... , if_n]\n",
    "        :param sample_weight: vector of importance weights for each observed interaction\n",
    "        :param epochs: number of training epochs (full passes through observed interactions)\n",
    "        :param verbose: whether to print epoch number and log-likelihood during training\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "\n",
    "        self._reset_state()\n",
    "        self.fit_partial(interactions, user_features, item_features, sample_weight, epochs, verbose)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def fit_partial(self, interactions, user_features=None, item_features=None, sample_weight=None, epochs=1, verbose=False):\n",
    "        \"\"\"learn or update model weights using the input data and resuming from the current model state\n",
    "\n",
    "        :param interactions: dataframe of observed user/item interactions: [user_id, item_id]\n",
    "        :param user_features: dataframe of user metadata features: [user_id, uf_1, ... , uf_n]\n",
    "        :param item_features: dataframe of item metadata features: [item_id, if_1, ... , if_n]\n",
    "        :param sample_weight: vector of importance weights for each observed interaction\n",
    "        :param epochs: number of training epochs (full passes through observed interactions)\n",
    "        :param verbose: whether to print epoch number and log-likelihood during training\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "\n",
    "        assert isinstance(epochs, int) and epochs >= 1, \"[epochs] must be a positive integer\"\n",
    "        assert isinstance(verbose, bool), \"[verbose] must be a boolean value\"\n",
    "\n",
    "        if self.is_fit:\n",
    "            self._init_interactions(interactions, sample_weight)\n",
    "            self._init_features(user_features, item_features)\n",
    "        else:\n",
    "            self._init_all(interactions, user_features, item_features, sample_weight)\n",
    "\n",
    "        # determine the number of negative samples to draw depending on the loss function\n",
    "        # NOTE: if [loss == 'bpr'] -> [max_samples == 1] and [multiplier ~= 1] for all updates\n",
    "        # NOTE: the [multiplier] is scaled by total number of items so it's always [0, 1]\n",
    "\n",
    "        if self.loss == 'bpr':\n",
    "            max_samples = 1\n",
    "        elif self.loss == 'warp':\n",
    "            max_samples = self.max_samples\n",
    "        else:\n",
    "            raise ValueError('[loss] function not recognized')\n",
    "\n",
    "        # NOTE: the cython private _fit() method updates the model weights in-place via typed memoryviews\n",
    "        # NOTE: therefore there's nothing returned explicitly by either method\n",
    "\n",
    "        _fit(\n",
    "            self.interactions,\n",
    "            self.sample_weight,\n",
    "            self.user_items,\n",
    "            self.x_uf,\n",
    "            self.x_if,\n",
    "            self.w_i,\n",
    "            self.w_if,\n",
    "            self.v_u,\n",
    "            self.v_i,\n",
    "            self.v_uf,\n",
    "            self.v_if,\n",
    "            self.alpha,\n",
    "            self.beta,\n",
    "            self.learning_rate,\n",
    "            self.learning_schedule,\n",
    "            self.learning_exponent,\n",
    "            max_samples,\n",
    "            epochs,\n",
    "            verbose\n",
    "        )\n",
    "\n",
    "        self.is_fit = True\n",
    "        return self\n",
    "\n",
    "\n",
    "    def __predict(self, pairs, cold_start='nan'):\n",
    "        \"\"\"calculate the predicted pointwise utilities for all (user, item) pairs\n",
    "\n",
    "        :param pairs: dataframe of [user, item] pairs to score\n",
    "        :param cold_start: whether to generate missing values ('nan') or drop ('drop') user/item pairs not found in training data\n",
    "        :return: np.array of real-valued model scores\n",
    "        \"\"\"\n",
    "\n",
    "        assert isinstance(pairs, (np.ndarray, pd.DataFrame)), \"[pairs] must be np.ndarray or pd.dataframe\"\n",
    "        assert pairs.shape[1] == 2, \"[pairs] should be: [user_id, item_id]\"\n",
    "        assert self.is_fit, \"you must fit the model prior to generating predictions\"\n",
    "\n",
    "        pred_pairs = pd.DataFrame(get_data(pairs).copy(), columns=['user_id', 'item_id'])\n",
    "        pred_pairs['user_id'] = pred_pairs['user_id'].map(self.user_to_index)\n",
    "        pred_pairs['item_id'] = pred_pairs['item_id'].map(self.item_to_index)\n",
    "        pred_pairs = np.ascontiguousarray(pred_pairs, dtype=np.float32)\n",
    "\n",
    "        scores = _predict(\n",
    "            pred_pairs,\n",
    "            self.x_uf,\n",
    "            self.x_if,\n",
    "            self.w_i,\n",
    "            self.w_if,\n",
    "            self.v_u,\n",
    "            self.v_i,\n",
    "            self.v_uf,\n",
    "            self.v_if\n",
    "        )\n",
    "\n",
    "        if cold_start == 'nan':\n",
    "            return scores\n",
    "        elif cold_start == 'drop':\n",
    "            return scores[~np.isnan(scores)]\n",
    "        else:\n",
    "            raise ValueError(\"param [cold_start] must be set to either 'nan' or 'drop'\")\n",
    "\n",
    "\n",
    "    def recommend(self, users, n_items=10, filter_previous=False, cold_start='nan'):\n",
    "        \"\"\"calculate the topN items for each user\n",
    "\n",
    "        :param users: iterable of user identifiers for which to generate recommendations\n",
    "        :param n_items: number of recommended items to generate for each user\n",
    "        :param filter_previous: remove observed training items from generated recommendations\n",
    "        :param cold_start: whether to generate missing values ('nan') or drop ('drop') users not found in training data\n",
    "        :return: pandas dataframe where the index values are user identifiers and the columns are recommended items\n",
    "        \"\"\"\n",
    "\n",
    "        assert getattr(users, '__iter__', False), \"[users] must be an iterable (e.g. list, array, series)\"\n",
    "        assert self.is_fit, \"you must fit the model prior to generating recommendations\"\n",
    "\n",
    "        user_idx = np.ascontiguousarray(pd.Series(users).map(self.user_to_index), dtype=np.float32)\n",
    "        rec_items = _recommend(\n",
    "            user_idx,\n",
    "            self.user_items,\n",
    "            n_items,\n",
    "            filter_previous,\n",
    "            self.x_uf,\n",
    "            self.x_if,\n",
    "            self.w_i,\n",
    "            self.w_if,\n",
    "            self.v_u,\n",
    "            self.v_i,\n",
    "            self.v_uf,\n",
    "            self.v_if\n",
    "        )\n",
    "        rec_items = pd.DataFrame(rec_items, index=users).apply(lambda c: c.map(self.index_to_item))\n",
    "\n",
    "        if cold_start == 'nan':\n",
    "            return rec_items\n",
    "        elif cold_start == 'drop':\n",
    "            return rec_items.dropna(how='any')\n",
    "        else:\n",
    "            raise ValueError(\"param [cold_start] must be set to either 'nan' or 'drop'\")\n",
    "\n",
    "\n",
    "    def similar_items(self, item_id, n_items=10):\n",
    "        \"\"\"find the most similar items wrt latent factor space representation\n",
    "\n",
    "        :param item_id: item to search\n",
    "        :param n_items: number of similar items to return\n",
    "        :return: np.array of topN most similar items wrt latent factor representations\n",
    "        \"\"\"\n",
    "\n",
    "        assert item_id in self.item_id.values, \"you must select an [item_id] present in the training data\"\n",
    "        assert self.is_fit, \"you must fit the model prior to generating similarities\"\n",
    "\n",
    "        try:\n",
    "            item_idx = self.item_to_index.loc[item_id]\n",
    "        except (KeyError, TypeError):\n",
    "            print(\"item_id={} not found in training data\".format(item_id))\n",
    "\n",
    "        # calculate item latent representations in F dimensional factor space\n",
    "        lr_item = self.v_i[item_idx] + np.dot(self.v_if.T, self.x_if[item_idx])\n",
    "        lr_all_items = self.v_i + np.dot(self.x_if, self.v_if)\n",
    "\n",
    "        # calculate the most similar N items excluding the search item\n",
    "        similarities = pd.Series(np.dot(lr_all_items, lr_item)).drop(item_idx).sort_values(ascending=False)[:n_items]\n",
    "        most_similar = pd.Series(similarities.index).map(self.index_to_item).values\n",
    "        return most_similar\n",
    "\n",
    "\n",
    "    def similar_users(self, user_id, n_users=10):\n",
    "        \"\"\"find the most similar users wrt latent factor space representation\n",
    "\n",
    "        :param user_id: user to search\n",
    "        :param n_users: number of similar users to return\n",
    "        :return: np.array of topN most similar users wrt latent factor representations\n",
    "        \"\"\"\n",
    "\n",
    "        assert user_id in self.user_id.values, \"you must select an [user_id] present in the training data\"\n",
    "        assert self.is_fit, \"you must fit the model prior to generating similarities\"\n",
    "\n",
    "        try:\n",
    "            user_idx = self.user_to_index.loc[user_id]\n",
    "        except (KeyError, TypeError):\n",
    "            print(\"user_id={} not found in training data\".format(user_id))\n",
    "\n",
    "        # calculate user latent representations in F dimensional factor space\n",
    "        lr_user = self.v_u[user_idx] + np.dot(self.v_uf.T, self.x_uf[user_idx])\n",
    "        lr_all_users = self.v_u + np.dot(self.x_uf, self.v_uf)\n",
    "\n",
    "        # calculate the most similar N users excluding the search user\n",
    "        similarities = pd.Series(np.dot(lr_all_users, lr_user)).drop(user_idx).sort_values(ascending=False)[:n_users]\n",
    "        most_similar = pd.Series(similarities.index).map(self.index_to_user).values\n",
    "        return most_similar\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        pass\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        return self.__predict(model_input.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b273b595-1006-4eee-a31f-eacb21dd7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.pyfunc.save_model(\n",
    "        path=\"example_mlflow_model\", python_model=RankFM(), artifacts=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e65de68-34e7-42e9-a27d-1334b436f9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model in `python_function` format\n",
    "loaded_model = mlflow.pyfunc.load_model(\"example_mlflow_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbdbb575-fd7a-4ecf-a3e2-d98ce9b407ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import log_metric, log_param, log_artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a05d0b67-d7d9-4d07-abc8-a8435c12b545",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_factors=20\n",
    "\n",
    "log_param(\"factor\",num_factors)\n",
    "\n",
    "model = RankFM(factors=num_factors, loss='warp', max_samples=20, alpha=0.01, sigma=0.1, learning_rate=0.10, learning_schedule='invscaling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03f5c2ec-b4ce-4d0b-8c76-7cb4977828d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training epoch: 0\n",
      "log likelihood: -434433.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RankFM at 0x7fe6baace910>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(interactions_train, epochs=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca973d36-ba94-45a1-89ab-7ad98bdf9d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.pyfunc.log_model(artifact_path=\"example_mlflow_model\", python_model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a07c5-96d9-4adb-ad03-8763e468f28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5070717c-096c-402f-8657-7dc160fb80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f65d39-2b4e-4649-bf11-c3b4b216111d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd3cec7-e809-4f6e-934b-29f889f099a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f8c296c-6da3-4fae-931e-b4373ab5fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperHero(object): #superclass, inherits from default object\n",
    "    def getName(self):\n",
    "        raise NotImplementedError #you want to override this on the child classes\n",
    "\n",
    "class SuperMan(SuperHero): #subclass, inherits from SuperHero\n",
    "    def getName(self):\n",
    "        return \"Clark Kent\"\n",
    "\n",
    "class SuperManII(SuperHero): #another subclass\n",
    "    def getName(self):\n",
    "        return \"Clark Kent, Jr.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a8808cc-0657-4db9-b466-4afcfeeeee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "superman=SuperMan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7f685472-35dd-4788-a515-ae2ae724989f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SuperMan at 0x7fe89abadc90>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "387c754b-8aa7-4654-bcc5-c26d4378734a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d6768cb7104c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"213\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-2b4d2b3c5a70>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, context, model_input)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'set' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "model.predict(None, model_input={\"213\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaba28d-ea78-4bf2-9fb3-e105175878d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4bab7164-2994-4154-a885-23b418a4487e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlflow.models.model.Model at 0x7fe89abc26d0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.pyfunc.Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeff09c-dc35-4e0b-9d04-a0048204c31d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8c6e3b2-c44f-4c30-bc65-5904ad08f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Required Packages and Set Options\n",
    "\n",
    "import os\n",
    "import sys\n",
    "# import git\n",
    "\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=nb.NumbaPerformanceWarning)\n",
    "\n",
    "#### Dynamically Re-Load all Package Modules\n",
    "\n",
    "import mlflow\n",
    "from mlflow import log_metric, log_param, log_artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "daa82a80-961f-4370-a2e7-968e3e628dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rankfm.rankfm import RankFM\n",
    "from rankfm.evaluation import hit_rate, reciprocal_rank, discounted_cumulative_gain, precision, recall, diversity\n",
    "\n",
    "# import neptune\n",
    "#import neptune_mlflow\n",
    "\n",
    "# neptune.init(project_qualified_name='myeonghak/example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7970079-13c1-452d-a265-b7a4a42a1dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interactions shape: (1000209, 3)\n",
      "interactions unique users: 6040\n",
      "interactions unique items: 3706\n",
      "user features users: 6040\n",
      "item features items: 3706\n",
      "interaction matrix sparsity: 95.5%\n",
      "train shape: (749724, 2)\n",
      "valid shape: (250485, 2)\n",
      "train users: 6040\n",
      "valid users: 6038\n",
      "cold-start users: set()\n",
      "train items: 3666\n",
      "valid items: 3531\n",
      "cold-start items: {3202, 2563, 644, 3460, 3209, 3722, 139, 396, 651, 1555, 1434, 1915, 1830, 2217, 2218, 687, 1842, 2742, 1851, 2619, 576, 3321, 3277, 3323, 3280, 601, 3290, 3164, 989, 865, 226, 3172, 1386, 878, 3187, 758, 3065, 763, 2556, 127}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((6040, 31), (3666, 19))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data\n",
    "path = \"/Users/a420777/Desktop/hcc/gnn/datasets/movielens/\"\n",
    "\n",
    "#Ratings\n",
    "interactions = pd.read_csv(path+'ratings.dat', sep='::', header=None, engine='python')\n",
    "interactions.columns = ['user_id','item_id','rating','timestamp']\n",
    "interactions = interactions.drop('timestamp', axis=1)\n",
    "\n",
    "#Movies\n",
    "item_names = pd.read_csv(path+'movies.dat', sep='::', header=None, engine='python')\n",
    "item_names.columns = ['item_id','title','genres']\n",
    "\n",
    "#Users\n",
    "user_features = pd.read_csv(path+'users.dat', sep='::', header=None, engine='python')\n",
    "user_features.columns = ['user_id','gender','age','occupation','zip-code']\n",
    "user_features = user_features.drop('zip-code', axis=1)\n",
    "\n",
    "# interactions = pd.read_csv(os.path.join(data_path, 'ML_1M_RATINGS.csv'))\n",
    "interactions.head()\n",
    "\n",
    "# user_features = pd.read_csv(os.path.join(data_path, 'ML_1M_USERS.csv'))\n",
    "user_features.occupation=user_features.occupation.astype(\"object\")\n",
    "user_features.age=user_features.age.astype(\"object\")\n",
    "\n",
    "user_features=pd.get_dummies(user_features)\n",
    "\n",
    "import collections\n",
    "\n",
    "split_series=item_names.genres.str.split(\"|\").apply(lambda x: x)\n",
    "\n",
    "split_series_dict=split_series.apply(collections.Counter)\n",
    "multi_hot=pd.DataFrame.from_records(split_series_dict).fillna(value=0)\n",
    "\n",
    "\n",
    "\n",
    "item_features=pd.concat([item_names.item_id, multi_hot], axis=1)\n",
    "\n",
    "\n",
    "#### Check Matrix/Vector Dimensions\n",
    "\n",
    "unique_users = interactions.user_id.nunique()\n",
    "unique_items = interactions.item_id.nunique()\n",
    "\n",
    "print(\"interactions shape: {}\".format(interactions.shape))\n",
    "print(\"interactions unique users: {}\".format(interactions.user_id.nunique()))\n",
    "print(\"interactions unique items: {}\".format(interactions.item_id.nunique()))\n",
    "\n",
    "print(\"user features users:\", interactions.user_id.nunique())\n",
    "print(\"item features items:\", interactions.item_id.nunique())\n",
    "\n",
    "#### Evaluate Interaction Matrix Sparsity\n",
    "\n",
    "sparsity = 1 - (len(interactions) / (unique_users * unique_items))\n",
    "print(\"interaction matrix sparsity: {}%\".format(round(100 * sparsity, 1)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(1492)\n",
    "interactions['random'] = np.random.random(size=len(interactions))\n",
    "test_pct = 0.25\n",
    "\n",
    "train_mask = interactions['random'] <  (1 - test_pct)\n",
    "valid_mask = interactions['random'] >= (1 - test_pct)\n",
    "\n",
    "interactions_train = interactions[train_mask][['user_id', 'item_id']]\n",
    "interactions_valid = interactions[valid_mask][['user_id', 'item_id']]\n",
    "\n",
    "train_users = np.sort(interactions_train.user_id.unique())\n",
    "valid_users = np.sort(interactions_valid.user_id.unique())\n",
    "cold_start_users = set(valid_users) - set(train_users)\n",
    "\n",
    "train_items = np.sort(interactions_train.item_id.unique())\n",
    "valid_items = np.sort(interactions_valid.item_id.unique())\n",
    "cold_start_items = set(valid_items) - set(train_items)\n",
    "\n",
    "print(\"train shape: {}\".format(interactions_train.shape))\n",
    "print(\"valid shape: {}\".format(interactions_valid.shape))\n",
    "\n",
    "print(\"train users: {}\".format(len(train_users)))\n",
    "print(\"valid users: {}\".format(len(valid_users)))\n",
    "print(\"cold-start users: {}\".format(cold_start_users))\n",
    "\n",
    "print(\"train items: {}\".format(len(train_items)))\n",
    "print(\"valid items: {}\".format(len(valid_items)))\n",
    "print(\"cold-start items: {}\".format(cold_start_items))\n",
    "\n",
    "user_features = user_features[user_features.user_id.isin(train_users)]\n",
    "item_features = item_features[item_features.item_id.isin(train_items)]\n",
    "user_features.shape, item_features.shape\n",
    "\n",
    "### Fit the Model on the Training Data and Evaluate Out-of-Sample Performance Metrics\n",
    "\n",
    "#### Initialize the Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab718864-3d9a-4c34-ab54-44ef3bccb65c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f2a8e8-6a27-48eb-b142-e35b851d5418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ca87c4-3b04-468c-8993-8119a19aa5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_scores = model.predict(interactions_valid, cold_start='nan') \n",
    "\n",
    "\n",
    "\n",
    "print(valid_scores.shape)\n",
    "pd.Series(valid_scores).describe()\n",
    "\n",
    "valid_recommendations = model.recommend(valid_users, n_items=10, filter_previous=True, cold_start='nan')\n",
    "valid_recommendations.head()\n",
    "\n",
    "#### Evaluate Model Performance on the Validation Data\n",
    "\n",
    "k = 10\n",
    "\n",
    "##### Generate Pure-Popularity Baselines\n",
    "\n",
    "most_popular = interactions_train.groupby('item_id')['user_id'].count().sort_values(ascending=False)[:k]\n",
    "most_popular\n",
    "\n",
    "test_user_items = interactions_valid.groupby('user_id')['item_id'].apply(set).to_dict()\n",
    "test_user_items = {key: val for key, val in test_user_items.items() if key in set(train_users)}\n",
    "\n",
    "base_hrt = np.mean([int(len(set(most_popular.index) & set(val)) > 0)                       for key, val in test_user_items.items()])\n",
    "base_pre = np.mean([len(set(most_popular.index) & set(val)) / len(set(most_popular.index)) for key, val in test_user_items.items()])\n",
    "base_rec = np.mean([len(set(most_popular.index) & set(val)) / len(set(val))                for key, val in test_user_items.items()])\n",
    "\n",
    "\n",
    "log_metric(\"hit_rate\", base_hrt)\n",
    "\n",
    "\n",
    "\n",
    "print(\"number of test users: {}\".format(len(test_user_items)))\n",
    "print(\"baseline hit rate: {:.3f}\".format(base_hrt))\n",
    "print(\"baseline precision: {:.3f}\".format(base_pre))\n",
    "print(\"baseline recall: {:.3f}\".format(base_rec))\n",
    "\n",
    "\n",
    "\n",
    "model_hit_rate = hit_rate(model, interactions_valid, k=k)\n",
    "model_reciprocal_rank = reciprocal_rank(model, interactions_valid, k=k)\n",
    "model_dcg = discounted_cumulative_gain(model, interactions_valid, k=k)\n",
    "model_precision = precision(model, interactions_valid, k=k)\n",
    "model_recall = recall(model, interactions_valid, k=k)\n",
    "\n",
    "\n",
    "\n",
    "print(\"hit_rate: {:.3f}\".format(model_hit_rate))\n",
    "print(\"reciprocal_rank: {:.3f}\".format(model_reciprocal_rank))\n",
    "print(\"dcg: {:.3f}\".format(model_dcg, 3))\n",
    "print(\"precision: {:.3f}\".format(model_precision))\n",
    "print(\"recall: {:.3f}\".format(model_recall))\n",
    "\n",
    "\n",
    "\n",
    "# recommendation_diversity = diversity(model, interactions_valid, k=k)\n",
    "# recommendation_diversity.head(10)\n",
    "\n",
    "# top_items = pd.merge(item_names, recommendation_diversity, on='item_id', how='inner')\n",
    "# top_items = top_items.set_index('item_id').loc[recommendation_diversity.item_id].reset_index()\n",
    "# top_items = top_items[['item_id', 'cnt_users', 'pct_users', 'title', 'genres']]\n",
    "# top_items.head(10)\n",
    "\n",
    "# coverage = np.mean(recommendation_diversity['cnt_users'] > 0)\n",
    "# print(\"percentage of items recommended to at least one user: {:.3f}\".format(coverage))\n",
    "\n",
    "# nonzero_users = recommendation_diversity[recommendation_diversity.cnt_users > 0]\n",
    "# entropy = -np.sum(nonzero_users['pct_users'] * np.log2(nonzero_users['pct_users']))\n",
    "# print(\"entropy value of recommended items: {:.3f}\".format(entropy))\n",
    "\n",
    "# N = 50\n",
    "# fig, axes = plt.subplots(1, 1, figsize=[16, 4])\n",
    "\n",
    "# topN = recommendation_diversity.iloc[:N, :]\n",
    "# axes.bar(topN.index.values + 1, topN.pct_users, width=1, edgecolor='black', alpha=0.75)\n",
    "# axes.set(xlabel='Item Rank', ylabel='Percentage of Users', title='Percentage of Users Recommended by Item Rank')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
